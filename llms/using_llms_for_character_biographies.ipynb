{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7db86073-956e-444d-a3df-cef9e9ec9e05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Note to Reviewer:\n",
    "I have not yet used LLMs in a corporate setting, but I have done training and have learned the fundamentals on my own. I know that GenAI will be a focus of this role. I wanted to provide sample code to demonstrate my skills in this area. I will be able to leverage my knowledge and training obtained outside of a corporate setting to more quickly learn on the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0b6135-714a-4a66-958a-98934cfcd33d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import GutenbergLoader\n",
    "\n",
    "from trulens_eval import (\n",
    "    Feedback,\n",
    "    TruChain,\n",
    "    Tru,\n",
    ")\n",
    "from trulens_eval.app import App\n",
    "from trulens_eval.feedback.provider.langchain import Langchain\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b315489-0431-46da-aca6-1f1accb454f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading in a dataset with main characters from each novel so I can perform extensive evaluation of models\n",
    "character_df = pd.read_csv(\"/dbfs/mnt/finance_tables/alexandre_dumas_characters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97587059-84ee-4a30-bab9-9534c2f91e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "character_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7dba4a1-f904-4992-8aea-2020e7babc0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_llm_name = \"databricks-dbrx-instruct\"\n",
    "eval_llm_name = \"databricks-mixtral-8x7b-instruct\"\n",
    "embedding_name = \"databricks-gte-large-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9644dbd6-8cfb-4bb7-aec5-f3a3dbd6625b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###### Setting embedding models\n",
    "db_embedding_model = DatabricksEmbeddings(endpoint=embedding_name)\n",
    "\n",
    "###### Setting llms for prediction and evaluation\n",
    "db_llm = ChatDatabricks(endpoint=chat_llm_name, temperature=0)\n",
    "db_eval_llm = ChatDatabricks(endpoint=eval_llm_name, temperature=0)\n",
    "\n",
    "# Using a different model for evaluation than for generating the models to avoid bias\n",
    "# Need a langchain object for trulens model evaluation\n",
    "langchain_provider = Langchain(\n",
    "    chain=ChatDatabricks(endpoint=eval_llm_name, temperature=0)\n",
    ")\n",
    "\n",
    "# Using semantic splitter solely for this use case because non-semantic alternatives did not perform well in v1\n",
    "text_splitter = SemanticChunker(\n",
    "    db_embedding_model, \n",
    "    breakpoint_threshold_type=\"standard_deviation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c163b56-020f-4fdf-8e38-5cc03ccf3daa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding book title metadata to help the model find the correct context\n",
    "book_dict = {\n",
    "    \"1257\": \"The Three Musketeers\",\n",
    "    \"1184\": \"The Count of Monte Cristo\",\n",
    "    \"1259\": \"Twenty Years After\",\n",
    "    \"2759\": \"The Man in the Iron Mask\",\n",
    "    \"965\": \"The Black Tulip\",\n",
    "}\n",
    "\n",
    "\n",
    "# Performing clean up of text to improve quality\n",
    "def clean_section(txt):\n",
    "    txt = re.sub(r\"\\n|\\r\", \" \", txt)\n",
    "    return re.sub(\" +\", \" \", txt)\n",
    "\n",
    "\n",
    "# Cleaning and chunking data\n",
    "processed_documents = []\n",
    "\n",
    "for document_number in list(book_dict.keys()):\n",
    "    loader = GutenbergLoader(\n",
    "        f\"https://www.gutenberg.org/cache/epub/{document_number}/pg{document_number}.txt\"\n",
    "    )\n",
    "    data = loader.load()\n",
    "    document_text = clean_section(data[0].page_content)\n",
    "    documents = text_splitter.create_documents([document_text])\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"book_title\"] = book_dict[document_number]\n",
    "    processed_documents.append(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ce35f6-d8b4-44df-acbb-dd02ef7a7596",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Putting vector index into ChromaDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=processed_documents[0], embedding=db_embedding_model\n",
    ")\n",
    "vectorstore.aadd_documents(processed_documents[1:5])\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce778911-3683-4bfa-9f16-f064e4988fb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prompt building:\n",
    "- Adding an example to the prompt greatly improved the model responses\n",
    "- Added visual cues for the user inputs\n",
    "- Outlined the steps the model should take to build the prompt\n",
    "- Directed the model to not make up information to reduce hallucinations\n",
    "- First versions had a lot of generic adjectives and character descriptions. I told the LLM to instead focus on key plot elements to improve the quality of the biographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a0d5dd-c77f-43a8-8902-d41b359f348d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the prompt template with multiple user-provided input variables\n",
    "custom_prompt_template = \"\"\"\n",
    "You are an AI assistant that writes biographies for book characters. The user will\n",
    "provide you with a character name. When given a character and book title,\n",
    "use the given context to create a short biography of the character from that book. Use the following steps:\n",
    "1. Use the document metadata to identify the appropriate document\n",
    "2. Once the correct document is identified, determine the main plot events this character was involved in\n",
    "3. Then use these plot events to construct a biography using the given example to show what kind of information and what style to use.\n",
    "\n",
    "If you are not familiar with the character provided, do not make up information.\n",
    "\n",
    "Use only the given context to build the biography:\n",
    "#### Context: {context}\n",
    "\n",
    "Create a biography for the following character:\n",
    "#### Character name: {character_name} \n",
    "\n",
    "Search for context from the following book:\n",
    "#### Book title: {book_title}. \n",
    "\n",
    "Use the following example to generate output: \n",
    "#### Example: Edmond Dantès: A Biography\n",
    "\n",
    "Edmond Dantès, the protagonist of Alexandre Dumas' novel \"The Count of Monte Cristo,\" is a young sailor who is betrayed by his friends and imprisoned for a crime he did not commit. Born in Marseille, France, Dantès is the son of a Bonapartist admiral and is engaged to be married to his father's ward, Mercédès.\n",
    "\n",
    "Dantès' life takes a drastic turn when he is falsely accused of treason and imprisoned in the Château d'If, a notorious island prison. While in prison, Dantès meets a fellow prisoner, Abbé Faria, who becomes his mentor and teaches him about the world outside of his cell.\n",
    "\n",
    "After Faria's death, Dantès finds a hidden treasure that Faria had been searching for, and he uses it to escape from prison and start a new life. Adopting the persona of the wealthy and mysterious Count of Monte Cristo, Dantès sets out to clear his name and seek revenge against those who wronged him.\n",
    "\n",
    "Throughout the novel, Dantès faces numerous challenges and obstacles as he navigates the complexities of high society and tries to uncover the truth about his past. Despite the many injustices he has suffered, Dantès remains a sympathetic and compelling character.\"\"\"\n",
    "\n",
    "# Create the PromptTemplate with the specified input variables\n",
    "input_variables = [\"character_name\", \"book_title\", \"context\"]\n",
    "PROMPT = PromptTemplate(\n",
    "    template=custom_prompt_template, input_variables=input_variables\n",
    ")\n",
    "\n",
    "# Specifying LLM chains for prediction and evaluation using different models\n",
    "llm_chain = LLMChain(prompt=PROMPT, llm=db_llm)\n",
    "eval_llm_chain = LLMChain(prompt=PROMPT, llm=db_eval_llm)\n",
    "\n",
    "# Call the chain with the required inputs. Doing one example for testing.\n",
    "character_name = \"Aramis\"\n",
    "book_title = \"The Three Musketeers\"\n",
    "result = llm_chain.run(\n",
    "    {\"character_name\": character_name, \"book_title\": book_title, \"context\": retriever}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad9ba632-995b-4a0c-9204-bce1a2fc11f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Trulens Triad\n",
    "### Context Relevance\n",
    "The first step of any RAG application is retrieval; to verify the quality of our retrieval, we want to make sure that each chunk of context is relevant to the input query. This is critical because this context will be used by the LLM to form an answer, so any irrelevant information in the context could be weaved into a hallucination. TruLens enables you to evaluate context relevance by using the structure of the serialized record.\n",
    "\n",
    "### Groundedness\n",
    "After the context is retrieved, it is then formed into an answer by an LLM. LLMs are often prone to stray from the facts provided, exaggerating or expanding to a correct-sounding answer. To verify the groundedness of our application, we can separate the response into individual claims and independently search for evidence that supports each within the retrieved context.\n",
    "\n",
    "### Answer Relevance\n",
    "Last, our response still needs to helpfully answer the original question. We can verify this by evaluating the relevance of the final response to the user input.\n",
    "\n",
    "Source: https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2510aff6-c96d-49e6-b014-960f57fcf409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create trulens evaluation function\n",
    "# Would normally put this in a separate .py file but I thought putting in a notebook would speed up review.\n",
    "def run_tru_evaluation(\n",
    "    app_id_name,\n",
    "    character_df,\n",
    "    langchain_provider,\n",
    "    llm_chain,\n",
    "    retriever,\n",
    "    reset_database=False,\n",
    "):\n",
    "    tru = Tru()\n",
    "\n",
    "    if reset_database:\n",
    "        tru.reset_database()\n",
    "\n",
    "    # select context to be used in feedback\n",
    "    context = App.select_context(retriever)\n",
    "\n",
    "    # Define a groundedness feedback function\n",
    "    groundedness = (\n",
    "        Feedback(\n",
    "            langchain_provider.groundedness_measure_with_cot_reasons,\n",
    "            name=\"Groundedness\",\n",
    "        )\n",
    "        .on(context.collect())\n",
    "        .on_output()\n",
    "    )\n",
    "\n",
    "    # Question/answer relevance between overall question and answer\n",
    "    answer_relevance = Feedback(\n",
    "        langchain_provider.relevance, name=\"Answer Relevance\"\n",
    "    ).on_input_output()\n",
    "    # Question/statement relevance between question and each context chunk\n",
    "    context_relevance = (\n",
    "        Feedback(langchain_provider.qs_relevance, name=\"Context Relevance\")\n",
    "        .on_input()\n",
    "        .on(context)\n",
    "        .aggregate(np.mean)\n",
    "    )\n",
    "\n",
    "    tru_recorder = TruChain(\n",
    "        llm_chain,\n",
    "        app_id=app_id_name,\n",
    "        feedbacks=[answer_relevance, context_relevance, groundedness],\n",
    "    )\n",
    "\n",
    "    # Loop through characters and book titles for evaluation\n",
    "    for index, row in character_df.iterrows():\n",
    "        character_name = row[\"character\"]\n",
    "        book_title = row[\"title\"]\n",
    "\n",
    "        with tru_recorder as recording:\n",
    "            llm_chain.invoke(\n",
    "                input={\n",
    "                    \"character_name\": character_name,\n",
    "                    \"book_title\": book_title,\n",
    "                    \"context\": retriever,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    record = None\n",
    "    if \"get_ipython\" in globals():\n",
    "        record = recording.get()\n",
    "\n",
    "    for feedback, feedback_result in record.wait_for_feedback_results().items():\n",
    "        feedback_value = feedback_result.result\n",
    "        if isinstance(feedback_value, int):\n",
    "            feedback_value = float(feedback_value)\n",
    "\n",
    "    records, feedback = tru.get_records_and_feedback(app_ids=[app_id_name])\n",
    "\n",
    "    records[\"book_title\"] = character_df[\"character\"].reset_index(drop=True)\n",
    "    records[\"character\"] = character_df[\"title\"].reset_index(drop=True)\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74056240-a51a-4a60-81ed-3dc132b0ae2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "app_id_name = \"CharacterBiographies\"\n",
    "\n",
    "# Performing evaluation\n",
    "records = run_tru_evaluation(\n",
    "    app_id_name,\n",
    "    character_df,\n",
    "    langchain_provider,\n",
    "    eval_llm_chain,\n",
    "    retriever,\n",
    "    reset_database=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be8e7a0-4a9d-46dd-94de-0e9452618412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Printing results\n",
    "records.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee23fb6-56ef-4819-b49f-38214ec91704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Showing leaderboard\n",
    "tru = Tru()\n",
    "\n",
    "tru.get_leaderboard(app_ids=[app_id_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cee0b18e-a6ce-4886-a804-611050faf8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Results\n",
    "| Model | Threshold Type | Embedding Model | Answer Relevance | Context Relevance | Groundedness | latency |\n",
    "| -------- | ------- | ------- | ------- | ------- | ------- | ------- |\n",
    "| DBRX | Percentile | BGE | 0.983333 | 0.9025 | 0.873267 | 3.191667 |\n",
    "| Llama3 | Percentile | BGE | 0.988542 | 0.870312 | 0.906493 | 3.005208 |\n",
    "| Llama2 | Percentile | BGE | 0.986905 | 0.877976 | 0.89929 | 3.02381 |\n",
    "| Mixtral | Percentile | BGE | 0.979167 | 0.926042 | 0.850545 | 3.358333 |\n",
    "| DBRX | Percentile | GTE | 0.958333 | 0.958333 | 0.855829 | 3.5 |\n",
    "| DBRX | Std Dev | GTE | 0.979167 | 0.96875 | 0.835575 | 3.416667 |\n",
    "| DBRX | IQR | GTE | 0.972222 | 0.965278 | 0.811165 | 3.361111 |\n",
    "\n",
    "### Best Model:** \n",
    "- **LLM**: databricks-dbrx-instruct performed best\n",
    "- **Embedding model**: databricks-gte-large-en performed substantially better than databricks-bge-large-en\n",
    "- **Breakpoint threshold type**: Using the standard deviation for identifying similar sentences during chunking performed slightly better than percentile and interquartile range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2efba1-f44f-43b5-9982-8328f9c79cbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Decision Making in this Code:\n",
    "- Used **langchain instead of llamaindex**\n",
    "  - Llamaindex puts a lot of its functionality \"underneath the hood\". For example, it uses OpenAI by default, and when you turn it off, using an alternate tokenizer does not accurately chunk data into properly sized chunks\n",
    "  - I saw much improved scalability and performance when using langchain versus llamaindex\n",
    "- I chose a **semantic data chunker** for this code.\n",
    "  - I started off with llamaindex's advanced RAG functions (sentence chunker, window retrieval, etc.) but they had several downsides:\n",
    "    - They used substantially more memory and often wouldn't finish on a reasonably-sized cluster\n",
    "    - They had a trouble extracting relevant context\n",
    "- I used **visual cues in my prompt** to indicate where user-defined inputs and context were located\n",
    "- Langchain has a **PromptTemplate functionality** that helped clean up the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c076ad53-da2d-429b-aee0-b2395347e107",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next Steps for Model Improvement\n",
    "- **Ground Truth Evaluation:** I would hand-create some \"golden samples\" that could be used for ground-truth evaluation to supplement the generic Trulens functions. I would also create my own custom evaluation metric and grading rubric. MLflow has a method that can then be used with an LLM-as-a-judge to evaluate the model with the custom rubric.\n",
    "- **Human Evaluation:** I would perform more extensive human quality evaluation with a subject-matter expert.\n",
    "- **Model Tuning:** I would do more extensive hyperparameter tuning. For example, I would adjust the model context (i.e. number of similar pieces of context to obtain from the vector index). I would also experiment with long-context reorder post-processing so the middle of the books might be given more attention.\n",
    "- **Few Shot Learning:** I would utilize model memory to give more examples on how to create good biographies so the model could better pick up on the correct writing style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1a3563-d7b1-42f4-93fa-0e8e9f19ef6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1162293504680036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "using_llms_for_character_biographies",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
