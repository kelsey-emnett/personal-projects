{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc46ac4-2036-4444-891e-b7dabdf5770f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from data_preparation import (\n",
    "    split_data,\n",
    "    preprocess_data,\n",
    "    id_highly_correlated_variables,\n",
    "    calc_mutual_info_scores,\n",
    "    remove_highly_correlated_vars,\n",
    ")\n",
    "from feature_selection import recursive_feature_selector, feature_importance_selector\n",
    "from train_model import fit_model\n",
    "from hyperopt_utils import Hyperopt\n",
    "from databricks.sdk.runtime import spark\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97acf9c-5787-4a1f-b28e-b682c94aea6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./configs/config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "data_path = config[\"data_path\"]\n",
    "target_col = config[\"target_col\"]\n",
    "seed = config[\"seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf59689-8845-4a15-b1d7-f063125e2680",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = spark.read.parquet(data_path).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0e865b-a267-4873-8f57-86b0e8543177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Row count is small. I am going to use sklearn for modeling instead of spark because it will have a larger variety of choices\n",
    "pandas_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd44ab6-6149-4706-85f8-d0ae6925a4f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690e8662-8acb-419b-8e6b-87140e3d0670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# appears from the above that the data types came in correctly\n",
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ffa7437-365d-4220-b54c-6c7276601d12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c16ea8a-91c7-492e-8b90-85cb48dee5be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# It appears that the dataset is pre-imputed and includes a missing flag whereever the missing data has occurred\n",
    "na_columns = pandas_df.columns[24:35]\n",
    "pandas_df[na_columns].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3155a3a6-22bd-4eb9-9ff0-cb8e2a29858f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data summary observations\n",
    "- Price the target variable is very skewed towards higher values, which may make correctly predicting higher prices difficult. However, given numbers shown below Airbnb likely gets most of its revenue from low-to-middle priced Airbnbs, which may mean we want to better predict those values rather than the tails. This means MAE or MAPE may be a better error metric for evaluation than RMSE or MSE, which will focus on predicting outliers well.\n",
    "- property_type and neighborhood_cleansed have very high cardinality. One-hot-encoding will not be a good method for these features.\n",
    "- There appear to be no obvious ordinal categorical variables in the category or numeric columns\n",
    "- Most obvious data cleaning has already been completed by Databricks. Mostly just feature engineering is left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e24d5a3-271d-4caf-abbb-44d60a17e2f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(pandas_df)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec027db5-bf12-4a3d-a683-926c9a6b3b89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c914ea1-4fc3-4c2c-a9c1-e6af85b4e33a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## High cardinality variables\n",
    "- For property_type, values are grouped into a small number of values and there is a long tail. I will recode as major types and create an \"other\" category\n",
    "- For neighborhood_cleansed, the values are spread out more evently within categories. A leave-one-out target encoding strategy may work better here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abe049d-84d7-4938-a65e-e25ece2f55da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df[\"property_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc7ef5b-a7fe-43a9-b3aa-aa5e17333813",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df[\"neighbourhood_cleansed\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a500af-d7b7-4810-84c0-33d856544e64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group smaller property types into a larger \"other\" category\n",
    "keep_property_values = list(\n",
    "    pandas_df[\"property_type\"].value_counts()[lambda x: x > 80].index\n",
    ")\n",
    "pandas_df[\"property_type\"] = pandas_df[\"property_type\"].apply(\n",
    "    lambda x: x if x in keep_property_values else \"Other\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd5db1d-38cf-48dd-bfc7-6fa7e8166647",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Choice of loss metric\n",
    "As shown during the EDA, Airbnb's rates are highly skewed. The majority  of Airbnb's revenue likely comes where the bulk of their rates are, low and medium. Only a small number of listings had very high rates. \n",
    "\n",
    "I wanted to predict lower and medium rates better so chose MAE as my loss metric. This metric does not penalize models more harshly for missing higher values. MSE would have been a better choice if predicting higher rates was more important to the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b051f035-d2c6-4f0a-8541-5e2b3d528e84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I'm going to work with the default missing value imputation.\n",
    "# It appears an intelligent method was used other than the most simple imputation method.\n",
    "modeling_df = pandas_df.drop(columns=na_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b64655-03ce-4541-bd6f-e875c9a21f00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b692e56-72da-46dd-a4bd-6928d67648a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split dataframe into training, validation, and test data sets\n",
    "# Separating target column from features\n",
    "train, y_train, val, y_val, test, y_test = split_data(modeling_df, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506611ef-ba8e-4820-ad28-6d9e3f81def4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Train data row count is {train.shape[0]}\")\n",
    "print(f\"Validation data row count is {val.shape[0]}\")\n",
    "print(f\"Holdout/test data row count is {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be9d555-0010-4d14-b04c-ce6567b2fc57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create lists of features for different data types\n",
    "high_card_vars = [\"neighbourhood_cleansed\"]\n",
    "cat_vars = list(\n",
    "    set(train.select_dtypes(include=[\"object\"]).columns.to_list()) - set(high_card_vars)\n",
    ")\n",
    "cont_vars = train.select_dtypes(include=[\"float64\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51e32b0-127f-45b8-8154-0d021a3eea00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model Tracking\n",
    "I prefer to use **MLflow** for model tracking and evaluation. It allows you to visualize performance of the different models versus their hyperparameters to better make trade offs. It also prevents duplicated work. Databricks also has some really awesome new capabilities to incorporate models with feature stores and data lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026c7417-cbf9-4efc-856b-ae4e7b9b7b8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.exceptions import RestException\n",
    "\n",
    "# Start MLflow experiment\n",
    "experiment_path = \"/Users/contact@kelseyhuntzberry.com/databricks-coding-challenge/mlflow_experiments/\"\n",
    "experiment_name = \"airbnb_price_prediction\"\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(f\"{experiment_path}{experiment_name}\")\n",
    "    mlflow.set_experiment(experiment_name=f\"{experiment_path}{experiment_name}\")\n",
    "except RestException:\n",
    "    mlflow.set_experiment(experiment_name=f\"{experiment_path}{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98870fa-5288-4822-8cec-604dd3dd025d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eabb8eb9-60d8-4e1e-a61d-ab59a1f489b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Send data through preprocessing pipeline\n",
    "(\n",
    "    preprocessor,\n",
    "    X_train,\n",
    "    X_val,\n",
    "    X_test,\n",
    "    cat_model_names,\n",
    "    num_model_names,\n",
    "    hc_model_names,\n",
    ") = preprocess_data(train, y_train, val, test, cat_vars, cont_vars, high_card_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb686fc4-f081-4067-89b2-ba1e5489b7fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bfe15cb-dc5d-4228-b69f-052a8a5cf054",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify highly correlated variables above the user-defined threshold\n",
    "cont_var_names = num_model_names + hc_model_names\n",
    "\n",
    "high_corr_df = id_highly_correlated_variables(X_train, cont_var_names, corr_cutoff=0.7)\n",
    "\n",
    "high_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58b2eb0-662f-453a-8e13-4bb3c53edcf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate mutual information scores\n",
    "mi_score_df = calc_mutual_info_scores(X_train, y_train, cont_var_names)\n",
    "\n",
    "mi_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4b29b3-e4fa-4c8f-86f5-af69d067bf8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing variables with the highest mutual information score that are above the correlation threshold\n",
    "corr_mi_score_df, remove_variables = remove_highly_correlated_vars(\n",
    "    high_corr_df, mi_score_df\n",
    ")\n",
    "\n",
    "print(\"remove the following variables:\", remove_variables)\n",
    "\n",
    "corr_mi_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7482b70-4369-4771-bdc2-d6d59f7ccfde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing highly correlated features\n",
    "X_train = X_train.drop(columns=remove_variables)\n",
    "X_val = X_val.drop(columns=remove_variables)\n",
    "X_test = X_test.drop(columns=remove_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "593f8609-7fcd-4e37-b967-34d2f56f4317",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Selection\n",
    "I am using two model-specific methods of feature selection, **recursive feature elimination** and a **feature importance** selector. These feature lists are then used in hyperopt hyperparameter tuning to improve model MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e691a22-d28a-4ecd-bc02-c052fd67fdd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting up a dictionary to cleanly perform feature selection without a lot of repeated code\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "feature_selection_model_dict = {\n",
    "    \"gbt\": GradientBoostingRegressor(random_state=seed, max_depth=5),\n",
    "    \"rf\": RandomForestRegressor(random_state=seed, max_depth=5),\n",
    "    \"ridge\": Ridge(random_state=seed),\n",
    "    \"lasso\": Lasso(random_state=seed),\n",
    "    \"elastic_net\": ElasticNet(random_state=seed),\n",
    "    \"decision_trees\": DecisionTreeRegressor(random_state=seed, max_depth=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b9f4a4-5fe5-46c8-847b-ef5cb7805966",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "\n",
    "# Performing feature selection for a variety of models\n",
    "for model_name in list(feature_selection_model_dict.keys()):\n",
    "    model = feature_selection_model_dict[model_name]\n",
    "\n",
    "    feature_names = recursive_feature_selector(model, X_train, y_train)\n",
    "\n",
    "    feature_set_name = f\"{model_name}_rfe\"\n",
    "\n",
    "    features_dict[feature_set_name] = feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62c8b78-21a1-408b-bbbd-93a20e9bf2c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performing feature selection for a variety of models\n",
    "for model_name in list(feature_selection_model_dict.keys()):\n",
    "    model = feature_selection_model_dict[model_name]\n",
    "\n",
    "    feature_names = feature_importance_selector(model, X_train, y_train)\n",
    "\n",
    "    feature_set_name = f\"{model_name}_importance\"\n",
    "\n",
    "    features_dict[feature_set_name] = feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d49812-92c0-4d29-a76c-320606cb0b87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./configs/features.json\", \"w\") as fp:\n",
    "    json.dump(features_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ec4d0a-2910-457e-8d9e-9bc8ced90375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d721660-b8a6-4fda-a78a-db2c52a2d0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "I am using the Hyperopt package that does parallelized hyperparameter tuning using Bayesian optimization for better scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99492e4c-3625-46c8-b388-414628f70d5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run hyperopt hyperparameter tuning across model types\n",
    "from hyperopt import space_eval\n",
    "\n",
    "hyperopt_class = Hyperopt(\n",
    "    X_train, y_train, X_val, y_val, features_dict, apply_overfit_penalty=True\n",
    ")\n",
    "\n",
    "best_result, search_space = hyperopt_class.run_hyperopt(max_evals=150)\n",
    "\n",
    "hyperopt_results = space_eval(search_space, best_result)\n",
    "print(hyperopt_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe143020-de92-44df-a7eb-5d1fde6956e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Final Evaluation\n",
    "Because Hyperopt does not log or optimize multiple evaluation metrics, I am doing a final evaluation with the validation and holdout datasets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44df3fc-e294-4258-a77d-a95d8aa2bb88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamically extract best hyperparameter values and correct model\n",
    "final_model_type = hyperopt_results[\"type\"]\n",
    "final_feature_list_name = hyperopt_results[\"feature_list\"]\n",
    "final_feature_list = features_dict[final_feature_list_name]\n",
    "\n",
    "final_model_params = hyperopt_results.copy()\n",
    "del final_model_params[\"type\"]\n",
    "del final_model_params[\"feature_list\"]\n",
    "\n",
    "final_model_params = hyperopt_class.round_hyperparameters(final_model_params)\n",
    "\n",
    "final_model = hyperopt_class.extract_model(\n",
    "    model_name=final_model_type, **final_model_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "301e0627-5254-4334-ac43-cacf1525cac6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Do full evaluation on model with validation data\n",
    "final_model, final_val_results, final_feature_importances = fit_model(\n",
    "    final_model,\n",
    "    features_dict[final_feature_list_name],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    final_model_type,\n",
    "    final_feature_list_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9f71e7-65c6-4ca9-947a-f1e77e060c10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c1e1a8-4474-487f-8d5a-cf2220490355",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3cb58d-4572-45aa-89c7-2b013a81a7f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation model with holdout framework\n",
    "holdout_model, holdout_results, holdout_feature_importances = fit_model(\n",
    "    final_model,\n",
    "    features_dict[final_feature_list_name],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    final_model_type,\n",
    "    final_feature_list_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5396f559-62d4-49d5-a377-8484e31ccc17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holdout_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b19d11-facb-440a-bce7-58485ad48104",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holdout_feature_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ca134d-8c68-44e5-af6c-dc28bc62dcc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Chosen Model:\n",
    "The model chosen above as the best model was random forest. It had the lowest MAE, which I chose to better predict the majority of listings.\n",
    "\n",
    "## Results Overview:\n",
    "The error of this model was quite high, which is likely driven by the low sample size of this dataset. In the entire dataset, there are only ~7k rows. There is also a long tail to the data, which is likely increasing error for the bulk of the dataset. \n",
    "\n",
    "## To Improve Model, Would Incorporate Business Goals:\n",
    "If given this dataset in a business setting, you would want to question stakeholders on whether predicting the \"bulk\" was more important than predicting the larger values. You would adjust the modeling methodology based on this feedback.\n",
    "\n",
    "If predicting the **high values** is more important, you would want to:\n",
    "- Change the loss metric for evaluation to RMSE which would reduce missing the target by a large amount\n",
    "\n",
    "If predicting the **low values and middle** is more important:\n",
    "- You would want to stay with the current loss metric\n",
    "- You could perform outlier removal of very high priced rentals\n",
    "- You could try outlier-resistant models such as Huber regression\n",
    "\n",
    "## Modeling Challenges: Overfitting\n",
    "When I removed the \"overfitting penalty\" from they hyperopt function, the evaluation metrics were improved, but the training evaluation metrics were much higher than the validation metrics. In this case, we want to ensure that the model would generalize to new data so I prioritized reducing overfitting above higher evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5e3c122-cb47-4869-9c11-7770bfe4cb85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78f1cdbb-02cf-4590-8d6c-06dff33ec09f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "airbnb_price_prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
